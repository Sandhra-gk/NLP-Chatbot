{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sandhra-gk/NLP-Chatbot/blob/main/TDA_Case_Study_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EugQFq-DjIFt"
      },
      "outputs": [],
      "source": [
        "#IGNORE\n",
        "\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU \\\n",
        "  transformers==4.31.0 \\\n",
        "  sentence-transformers==2.2.2 \\\n",
        "  pinecone-client==2.2.2 \\\n",
        "  accelerate==0.21.0 \\\n",
        "  einops==0.6.1 \\\n",
        "  langchain \\\n",
        "  xformers==0.0.20 \\\n",
        "  bitsandbytes==0.41.0"
      ],
      "metadata": {
        "id": "GewAejAVjPFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "embed_model = HuggingFaceEmbeddings(\n",
        "    model_name=embed_model_id,\n",
        "    model_kwargs={'device': device},\n",
        "    encode_kwargs={'device': device, 'batch_size': 32}\n",
        ")"
      ],
      "metadata": {
        "id": "OHE4w16JjVz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "    \"this is one document\",\n",
        "    \"and another document\"\n",
        "]\n",
        "\n",
        "embeddings = embed_model.embed_documents(docs)\n",
        "\n",
        "print(f\"We have {len(embeddings)} doc embeddings, each with \"\n",
        "      f\"a dimensionality of {len(embeddings[0])}.\")"
      ],
      "metadata": {
        "id": "_0DXUzsvjdTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall pinecone-client"
      ],
      "metadata": {
        "id": "i88IlPZOqfz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone-client==2.2.2"
      ],
      "metadata": {
        "id": "XtmqhMraDylV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# from pinecone import Pinecone, PodSpec\n",
        "import pinecone\n",
        "\n",
        "index_name = 'llama-2-rag'\n",
        "\n",
        "PINECONE_API_KEY = '4c2452a9-049f-4f4e-b198-6a45eb951a37'\n",
        "PINECONE_ENVIRONMENT = 'gcp-starter'\n",
        "\n",
        "pinecone.init(\n",
        "    api_key=PINECONE_API_KEY,\n",
        "    environment=PINECONE_ENVIRONMENT\n",
        ")\n",
        "\n",
        "\n",
        "index = pinecone.Index(index_name)\n",
        "\n",
        "index.describe_index_stats()"
      ],
      "metadata": {
        "id": "XO33IRa8jgs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n"
      ],
      "metadata": {
        "id": "SBOflRETEiYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DON'T RUN\n",
        "\n",
        "from datasets import load_dataset\n",
        "data = load_dataset(\n",
        "    'csv', data_files=['dataset.csv'], split='train')\n",
        "\n",
        "print(data)\n",
        "\n",
        "data = data.to_pandas()\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "id": "ru3qhy7vjkp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DON'T RUN\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "    i_end = min(len(data), i+batch_size)\n",
        "    batch = data.iloc[i:i_end]\n",
        "    ids = [f\"{x['Title']}\" for i, x in batch.iterrows()]\n",
        "    title = [x['Title'] for i, x in batch.iterrows()]\n",
        "    texts = [x['Abstract'] for i, x in batch.iterrows()]\n",
        "    authors = [x['Authors'] for i, x in batch.iterrows()]\n",
        "    embeds = embed_model.embed_documents(texts)\n",
        "    metadata = [{'title': x['Title'], 'authors': x['Authors']}\n",
        "                for i, x in batch.iterrows()]\n",
        "    try:\n",
        "        # add to Pinecone\n",
        "        index.upsert(vectors=zip(ids, embeds, metadata))\n",
        "    except Exception as e:\n",
        "        print(\"Error processing batch:\")\n",
        "        print(batch)\n",
        "        print(f\"Error message: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "lTxHl2SZjnlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DON'T RUN\n",
        "\n",
        "import transformers\n",
        "from torch import cuda, bfloat16\n",
        "\n",
        "model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# begin initializing HF items, need auth token for these\n",
        "hf_auth = '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'\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    token=hf_auth\n",
        ")\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    token=hf_auth\n",
        ")\n",
        "model.eval()\n",
        "print(f\"Model loaded on {device}\")"
      ],
      "metadata": {
        "id": "3BJWWwU5js--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.31.0"
      ],
      "metadata": {
        "id": "q5ja19bGRyM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from torch import cuda, bfloat16\n",
        "\n",
        "\n",
        "model_id = 'openlm-research/open_llama_7b_v2'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# begin initializing HF items, need auth token for these\n",
        "hf_auth = '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'\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    token=hf_auth\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    token=hf_auth,\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        ")\n",
        "model.eval()\n",
        "print(f\"Model loaded on {device}\")"
      ],
      "metadata": {
        "id": "QbqfmOJVj7jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    token=hf_auth\n",
        ")"
      ],
      "metadata": {
        "id": "oHcHdStZkAS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text = transformers.pipeline(\n",
        "    model=model, tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")"
      ],
      "metadata": {
        "id": "Y15U6sMskDCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = generate_text(\"Explain to me the difference between nuclear fission and fusion.\")\n",
        "print(res[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "qV7SqJCykFud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)\n",
        "llm(prompt=\"Explain the paper Representation of professions in entertainment media\")"
      ],
      "metadata": {
        "id": "7IJUKm36kIgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "text_field = 'abstract'  # field in metadata that contains text content\n",
        "\n",
        "vectorstore = Pinecone(\n",
        "    index, embed_model.embed_query, text_field\n",
        ")"
      ],
      "metadata": {
        "id": "sBg6KvStkLMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'Explain the paper Representation of professions in entertainment media'\n",
        "\n",
        "vectorstore.similarity_search(\n",
        "    query,  # the search query\n",
        "    k=3  # returns top 3 most relevant chunks of text\n",
        ")"
      ],
      "metadata": {
        "id": "LNc9flR8kT4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "rag_pipeline = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type='stuff',\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "eX9O-gPxkW7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm('Who wrote the paper Representation of professions in entertainment media')"
      ],
      "metadata": {
        "id": "gvPG_utKkZud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline('Who wrote the paper Representation of professions in entertainment media')"
      ],
      "metadata": {
        "id": "FTp4EzDmkcbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm('What is the name of the paper written by Sabyasachee Baruah')"
      ],
      "metadata": {
        "id": "hEdIies6kfAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline('What is the name of the paper written by Sabyasachee Baruah')"
      ],
      "metadata": {
        "id": "QbSxZ5w5khTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline('Explain the paper jdcnowevbo')"
      ],
      "metadata": {
        "id": "mvlr1unkVv6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline('Who wrote the paper attention is all you need')"
      ],
      "metadata": {
        "id": "-81c6wIqkjj-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline('Who wrote the paper attention is all you need')\n",
        "x = rag_pipeline('''')\n",
        "start_index = text.find(x)\n",
        "if start_index != -1:\n",
        "    substring = x[start_index:]\n",
        "else:\n",
        "    substring = \"The word 'Question' was not found in the string.\"\n",
        "\n",
        "print(substring)"
      ],
      "metadata": {
        "id": "rykcwva5llqa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Define cosine similarity function\n",
        "def cosine_similarity(text1, text2):\n",
        "    # Create TF-IDF vectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Fit and transform the text data\n",
        "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarity_matrix = np.dot(tfidf_matrix, tfidf_matrix.T)\n",
        "\n",
        "    # Return the cosine similarity score\n",
        "    return similarity_matrix[0, 1]\n",
        "# Define your prompt\n",
        "prompt = \"Explain the paper ilhkugvskedvgk\"\n",
        "\n",
        "# Generate a response from the language model\n",
        "generated_response = rag_pipeline(prompt)\n",
        "generated_text = generated_response['result']\n",
        "\n",
        "# Convert the generated text to a string\n",
        "generated_text_str = str(generated_text)\n",
        "\n",
        "# Search for similar documents in the vector store\n",
        "results = vectorstore.similarity_search(\n",
        "    generated_text_str,  # the search query\n",
        "    k=3  # returns top 3 most relevant chunks of text\n",
        ")\n",
        "\n",
        "# Check if the generated response is hallucinated\n",
        "is_hallucinated = True\n",
        "similarity_score = 1\n",
        "for result in results:\n",
        "    temp = cosine_similarity(generated_text_str, result.page_content)\n",
        "\n",
        "    # If the similarity score is above a certain threshold, consider it not hallucinated\n",
        "    if temp < similarity_score:  # Adjust the threshold as needed\n",
        "        similarity_score = temp\n",
        "print(\"Cosine Similarity Score:\", similarity_score)\n",
        "if similarity_score > 0.5:\n",
        "  is_hallucinated = False\n",
        "# Print whether there is hallucination\n",
        "if is_hallucinated:\n",
        "    print(\"The generated response is potentially hallucinated.\")\n",
        "else:\n",
        "    print(\"The generated response is not hallucinated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Hp_spSLbOWw",
        "outputId": "45df3354-d084-4974-84d3-078a22bb754c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Score: 0.4626114277721045\n",
            "The generated response is potentially hallucinated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Define cosine similarity function\n",
        "def cosine_similarity(text1, text2):\n",
        "    # Create TF-IDF vectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Fit and transform the text data\n",
        "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarity_matrix = np.dot(tfidf_matrix, tfidf_matrix.T)\n",
        "\n",
        "    # Return the cosine similarity score\n",
        "    return similarity_matrix[0, 1]\n",
        "# Define your prompt\n",
        "prompt = \"Explain the paper Representation of professions in entertainment media\"\n",
        "\n",
        "# Generate a response from the language model\n",
        "generated_response = rag_pipeline(prompt)\n",
        "generated_text = generated_response['result']\n",
        "\n",
        "# Convert the generated text to a string\n",
        "generated_text_str = str(generated_text)\n",
        "\n",
        "# Search for similar documents in the vector store\n",
        "results = vectorstore.similarity_search(\n",
        "    generated_text_str,  # the search query\n",
        "    k=3  # returns top 3 most relevant chunks of text\n",
        ")\n",
        "\n",
        "# Check if the generated response is hallucinated\n",
        "is_hallucinated = True\n",
        "similarity_score = 1\n",
        "for result in results:\n",
        "    temp = cosine_similarity(generated_text_str, result.page_content)\n",
        "\n",
        "    # If the similarity score is above a certain threshold, consider it not hallucinated\n",
        "    if temp < similarity_score:  # Adjust the threshold as needed\n",
        "        similarity_score = temp\n",
        "print(\"Cosine Similarity Score:\", similarity_score)\n",
        "if similarity_score > 0.5:\n",
        "  is_hallucinated = False\n",
        "# Print whether there is hallucination\n",
        "if is_hallucinated:\n",
        "    print(\"The generated response is potentially hallucinated.\")\n",
        "else:\n",
        "    print(\"The generated response is not hallucinated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6G9Qpqgotdp",
        "outputId": "11807740-f703-4cb8-b545-9431fd8410a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Score: 0.6658136663773845\n",
            "The generated response is not hallucinated.\n"
          ]
        }
      ]
    }
  ]
}
